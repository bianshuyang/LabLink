{ "id": "0", "title": "Automating Biomedical Abstract Screening using Network Embedding", "author": "Eric Lee, Emory University", "date": "2023-09-28", "description": "Systematic review (SR) is an essential process to identify, evaluate, and summarize the findings of all relevant individual studies concerning health-related questions. However, conducting a SR is labor-intensive, as identifying relevant studies is a daunting process that entails multiple researchers screening thousands of articles for relevance. Automating evidence synthesis using machine learning models has been proposed but primarily focuses on the text and ignores additional features like citation information. Recent work demonstrated that citation embeddings can outperform the text itself, suggesting that better network representation may expedite SRs. Yet, how to utilize the rich information in heterogeneous information networks for network embeddings is understudied. Also, the lack of a unified source that includes the metadata of biomedical literature makes the research more challenging. To deal with this problem, we propose four works. First, we propose a model that exploits three representations, documents, topics, and citation networks to show the effectiveness of the additional features. Second, we introduce the PubMed Graph Benchmark, one of the largest heterogeneous networks to date, which aggregates the rich metadata into a unified source that includes abstracts, authors, citations, MeSH terms, etc. Third, we propose a heterogeneous network embedding model that uses a community-based multi-view graph convolutional network for learning better embeddings for evidence synthesis. Lastly, we propose a hierarchical network embedding model that uses the Poincare embedding model on the MeSH tree and the citation network.", "imageUrl": "images/test.jpg"}
{ "id": "1", "title": "Understanding and Incentivizing Behavior in Emerging Decentralized Ecosystems", "author": "Mark Ma, Emory University", "date": "2023-08-30", "description": "This dissertation explores the application of blockchain technology and the incentivization of behavior in emerging decentralized ecosystems. We investigate its application in the healthcare sector, with a particular focus on genomic data sharing. We also examine its potential in deterring illicit activities such as cryptocurrency fraud and explore its role in confidential tracking within decentralized delivery systems.\\\\\n\\\\\nWe propose efficient strategies for data storage and retrieval in blockchain systems, specifically targeting cross-site genomic data sharing. Our blockchain-based log system provides a lightweight and widely compatible module for existing blockchain platforms. By ensuring accountability in cross-site genomic data sharing, we demonstrate the feasibility of blockchain technology in incentivizing responsible behavior and enhancing collaboration across different healthcare entities.\\\\\n\\\\\nIn addressing the issue of illicit activities in the blockchain ecosystem, we design a virtual taint system that marks cryptocurrency transactions as \"tainted\" if they are known to be involved in crime, fraud, or other illicit activities. This system serves to disincentivize such activities, revitalizing integrity within the cryptocurrency ecosystem.\\\\\n\\\\\nFurthermore, we present a blockchain-based system for the confidential tracking of routes in decentralized delivery systems. By leveraging the transparency of blockchain while preserving business confidentiality, this system reveals necessary information only when fraud occurs, thereby incentivizing honest behavior and enhancing trust within the network.\nOur research contributes significantly to both the understanding and incentivization of behavior in emerging decentralized ecosystems, particularly within the context of blockchain technology. The findings pave the way for secure and efficient data management solutions across various sectors and contribute to the creation of a safer and more secure digital environment.", "imageUrl": "images/test.jpg"}
{ "id": "2", "title": "Cell Type Identification in Single-cell Genomics and its Applications", "author": "Wenjing Ma, Emory University", "date": "2023-04-27", "description": "Advances in techniques for measuring genomics in cell-level resolution provide great opportunities to reveal the cell heterogeneity. When dealing with single-cell genomics sequencing data, the most fundamental and critical step is to accurately identify cell types (celltyping). Once cell types are identified, the understanding of cell signatures, cellular composition and cell dynamics can broaden the knowledge of biological processes and potentially cure diseases. Traditional approaches of celltyping in single-cell genomics data are based on unsupervised clustering, expert’s knowledge, and manual curation, which are labor intensive. With continuous accumulation of high-quality single-cell genomics data, supervised celltyping becomes more and more popular due to its accuracy, robustness, and efficiency. In this seminar, the speaker will first introduce a benchmark study in supervised celltyping in single-cell RNA-sequencing data. Based on the experience gained from the benchmark study, the speaker will then introduce the supervised celltyping methods specifically developed for single-cell chromatin accessibility (scATAC-seq) data. Finally, with the accurately identified cell types, the speaker will introduce a method developed to identify cellular activity in bulk differential expression study with the usage of cell-type-specific marker information.", "imageUrl": "images/test.jpg"}
{ "id": "3", "title": "Defensive Machine Learning Techniques for Countering Adversarial Attacks", "author": "Fereshteh Razmi, Emory University", "date": "2023-04-20", "description": "The increasing reliance on machine learning algorithms has made them a target for exploiting vulnerabilities in these systems and launching adversarial attacks. The attacker in these attacks manipulates either the training data or test data, or both, known as a poisoning attack, adversarial example, or backdoor attack, respectively. They primarily aim to disrupt the model's classification task. In cases where the model is interpretable, the attacker may target the interpretation of the model's output. \n\nThese attacks can have significant negative impacts; therefore, it is crucial to develop effective defense methods to protect against them. Current defense methods have limitations. Outlier detectors, used to identify and mitigate poisoning attacks, require prior knowledge of the attack and clean data to train the detector. Robust defense methods show promising results in mitigating backdoor attacks, but their effectiveness comes at the cost of decreased model utility. Furthermore, few defense methods have addressed adversarial examples that target the interpretation of the model's output.\n\nTo address these limitations, we propose defense methods that protect machine learning models from adversarial attacks. Our methods include an autoencoder-based detection approach to identify various untargeted poisoning attacks. We also provide a comprehensive comparative study of differential privacy approaches and suggest new approaches based on label differential privacy to defend against backdoor attacks. Lastly, we propose a novel attack and defense method to protect the interpretation of a healthcare-related machine learning model. These approaches represent significant progress in the field of machine learning security and have the potential to protect against a wide range of adversarial attacks.\"", "imageUrl": "images/test.jpg"}
{ "id": "4", "title": "Contextual Embedding Representation for Dialogue Systems", "author": "Zihao Wang, Emory University", "date": "2023-03-30", "description": "Context is a crucial element for conversational agents to conduct natural and engaging conversations with human users. By being aware of the context, a conversational agent can capture, understand, and utilize relevant information, such as named entity mentions, topics of interest, user intents, and emotional semantics. However, incorporating contextual information into dialogue systems is a challenging task due to the various forms it can take, the need to decide which information is most relevant, and how to organize and integrate it.\n\nTo address these challenges, this thesis proposes exploring and experimenting with different contextual information in the embedding space across different models and tasks. Furthermore, the thesis develops models that overcome the limitations of state-of-the-art language models in terms of the maximum number of tokens they can encode and their incapacity to fuse arbitrary forms of contextual information. Additionally, diarization methods are explored to resolve speaker ID errors in the transcriptions, which is crucial for training dialogue data.\n\n \nThe proposed models address the challenges of context integration into retrieval-based and generation-based dialogue systems. In retrieval-based systems, a response is selected and returned by ranking all responses from different components. A contextualized conversational ranking model is proposed and evaluated on the MSDialog benchmark conversational corpus, where three types of contextual information are leveraged and incorporated into the ranking model: previous conversation utterances from both speakers, semantically similar response candidates, and domain information associated with each candidate response. The performance of the contextual response ranking model exceeded state-of-the-art models in previous research, showing the potential to incorporate various forms of context into modeling.\n\n\nIn generation-based systems, a generative model generates a response to be returned to the conversing party. A generative model is built on top of the Blenderbot model, overcoming its limitations to integrate two types of contextual information: previous conversation utterances from both conversing parties and heuristically identified stacked questions that tackle repetition and provide topical diversity in dialogue generations. The models are trained on an interview dataset and evaluated on an annotated test set by professional interviewers and students in real conversations. The average satisfaction score from professional interviewers and students is 3.5 out of 5, showing promising future applications.\n\n\nAdditionally, to better understand topics of interest, topical clustering and diversity are investigated by grouping topics and analyzing the topic flow in the interview conversations. Frequent occurrences of some clusters of topics give a clear presentation of what scopes of topics an interview would touch on while maintaining a great selection of unique topics for individuals. Based on this observation, another generative model architecture integrating topical information is proposed that generates the next topic of interest in the conversation flow in parallel to generating utterances. This work is ongoing, with the expectation of improving the performance of the previous generative model.\n\n\nDay/time: March 30th, 11:00 am - 12:30 pm\n\nRoom: Math CS E306\n\nZoom Option: \nhttps://us02web.zoom.us/j/9910064905?pwd=aThaYVd1eFBkRWpwQ2xibFFneHIzUT09\nMeeting ID: 991 006 4905\n\nPasscode: 290751", "imageUrl": "images/test.jpg"}
{ "id": "5", "title": "Computational Structures as Neural Symbolic Representation", "author": "Han He, Emory University", "date": "2023-03-30", "description": "Although end-to-end neural models have been dominating Natural Language Processing for both performance and flexibility, critics have recently drawn attention to their poor generalization and lack of interpretability. Conversely, symbolic paradigms such as Abstract Meaning Representation (AMR) are humanly comprehensible but less flexible. In response, we propose Executable Abstract Meaning Representation (EAMR) as a reconciliation of both paradigms. EAMR is a neural symbolic framework that frames a task as a program, which interactively gets generated, revised and executed. In our novel definition, execution is a sequence of transforms on AMR graphs. Through a hybrid runtime, EAMR learns the automatic execution of AMR graphs, yet it also allows for the integration of handcrafted heuristics, knowledge bases and APIs. EAMR can be used in many applications such as dialogue understanding and response generation.\n\n\nhttps://emory.zoom.us/j/91398763361", "imageUrl": "images/test.jpg"}
{ "id": "6", "title": "Attention-enhanced Deep Learning Models for Data Cleaning and Integration", "author": "Jing Zhang, Emory University", "date": "2023-03-28", "description": "Data cleaning and integration is an essential process for ensuring the accuracy and consistency of data used in analytics and decision-making. Schema matching and entity matching tasks are crucial aspects of this process to merge data from various sources into a single, unified view. Schema matching seeks to identify and resolve semantic differences between two or more database schemas whereas entity matching seeks to detect the same real-world entities in different data sources. Given recent deep learning trends, pre-trained transformers have been proposed to automate both the schema matching and entity matching processes. However, existing models only utilize the special token representation (e.g., [CLS]) to predict matches and ignore rich and nuanced contextual information in the description, thereby yielding suboptimal matching performance. To improve performance, we propose the use of the attention mechanism to (1) learn the schema matches between source and target schemas using the attribute name and description, (2) leverage the individual token representations to fully capture the information present in the descriptions of the entities, and (3) jointly utilize the attribute descriptions and entity descriptions to perform both schema and entity matching.", "imageUrl": "images/test.jpg"}
{ "id": "7", "title": "Enhancing Document Understanding through the Incorporation of Structural Inference", "author": "Liyan Xu, Emory University", "date": "2023-03-28", "description": "Towards resolving a variety of Natural Language Processing (NLP) tasks, pretrained language models (PLMs) have been incredibly successful by simply modeling language sequences, backed by their powerful sequence encoding capabilities. However, for document understanding tasks involving multi-sentence or multi-paragraph inputs, the model still needs to overcome the inherent challenge of processing scattered information across the entire document context, such as resolving pronouns or recognizing relations among multiple sentences.\n\n \n\nTo address the motivation of effectively understanding document context beyond sequence modeling, this dissertation presents an in-depth study on the incorporation of structural inference, utilizing intrinsic structures of languages and documents. Four research works are outlined within this dissertation. Particularly, the first work proposes to integrate syntactic dependency structures into the document encoding process, capturing inter-sentence dependencies through designed graph encoding for the task of machine reading comprehension, especially under the multilingual setting. The second work investigates different methods to perform inference on the discourse structure that concerns coreference relations, allowing for higher-order decision making. The third work presents a novel formulation of structural inference to facilitate joint information extraction, fusing multi-facet information of document entities in terms of both coreference and relations. The last work explores the potential of the sequence-to-sequence generation as an approach that performs implicit inference on linearized entity structures, motivated by its unified encoder-decoder architecture and inherent abilities to perform higher-order inference.\n\n\nOverall, this dissertation demonstrates that incorporating designed structural inference upon certain intrinsic structures of languages or documents can effectively enhance document understanding, and highlights that modeling dependencies among different parts of the context can lead to more accurate and robust encoding and decoding process, where auxiliary information can be provided that complements the sequence modeling of PLMs.\n\nZoom Option:  https://emory.zoom.us/j/95753738482", "imageUrl": "images/test.jpg"}
{ "id": "8", "title": "Few Shot Learning for Rare Disease Diagnosis", "author": "Emily Alsentzer, MIT & Harvard", "date": "2022-12-09", "description": "Rare diseases affect 300-400 million people worldwide, yet each disease affects no more than 50 per 100,000 individuals. Many patients with rare genetic conditions remain undiagnosed due to clinicians' lack of experience with the individual diseases and the considerable heterogeneity of clinical presentations.  Machine-assisted diagnosis offers the opportunity to shorten the diagnostic delays for rare disease patients. Recent advances in deep learning have considerably improved the accuracy of medical diagnosis. However, much of the success thus far is contingent on the availability of large, annotated datasets. Machine-assisted rare disease diagnosis necessitates that approaches learn from limited data and extrapolate beyond training distribution to novel genetic conditions. In this talk, I will present our work towards developing few-shot methods that can overcome the data limitations of deep learning to diagnose patients with rare genetic conditions. To infuse external knowledge into models, we first develop novel graph neural network methods for subgraph representation learning that encode how subgraphs (e.g., a set of patient phenotypes) relate to a larger knowledge graph. We leverage these advances to develop SHEPHERD, a geometric deep learning method that reasons over biomedical knowledge to diagnose patients with rare–even novel–genetic conditions. SHEPHERD operates at multiple facets throughout the rare disease diagnosis process: performing causal gene discovery, retrieving \"patients-like-me\", and providing interpretable characterizations of novel disease presentations. Our work demonstrates the potential for deep learning methods to accelerate the diagnosis of rare disease patients and has implications for the use of deep learning on medical datasets with very few labels.", "imageUrl": "images/test.jpg"}
{ "id": "9", "title": "Language Guided Localization and Navigation", "author": "Meera Hahn,", "date": "2022-12-02", "description": "Embodied tasks that require active perception are key to improving language grounding models and creating holistic social agents. In this talk we explore two multi-modal embodied perception tasks which require localization or navigation of an agent in an unknown 3D space with limited information about the environment. First we present the Where Are You? (WAY) dataset which contains over 6k dialogs of two humans performing a localization task. On top of this dataset, we propose the task of Localization from Embodied Dialog (LED). The LED task involves taking a natural language dialog of two agents -- an observer and a locator -- and predicting the location of the observer agent. The second task we examine is the Vision Language Navigation (VLN) task, in which an agent navigates via natural language instructions. For both tasks, we address the objective of improving model accuracy and demonstrate that this can be done using passive data, which can introduce more semantically rich and diverse information during training, in comparison to additional interaction data. We additionally introduce a novel analysis pipeline for both tasks to diagnose and reveal limitations and failure modes of these types of common multi-modal models.\n\n\nBiography:\n\nMeera Hahn is a Research Scientist at Google Research working on multi-modal modeling of vision and natural language for applications in artificial intelligence. Her long-term research goal is to develop multi-modal systems capable of supporting robotic or AR assistants that can seamlessly interact with humans. She recently completed her PhD in Computer Science at the Georgia Institute of Technology under Dr. James M. Rehg. Her research at Georgia Tech focused on training embodied agents (in simulation) to perform complex semantic grounding tasks.\n\n\n\nhttp://cs.emory.edu/home/", "imageUrl": "images/test.jpg"}
{ "id": "10", "title": "Interpretable and Interactive Representation Learning on Geometric Data", "author": "Yuyang Guo, Emory University", "date": "2022-12-01", "description": "In recent years, representation learning on geometrics data, such as image and graph-structured data, are experiencing rapid developments and achieving significant progress thanks to the rapid development of Deep Neural Networks (DNNs), including Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs). However, DNNs typically offer very limited transparency, imposing significant challenges in observing and understanding when and why the models make successful/unsuccessful predictions. While we are witnessing the fast growth of research in local explanation techniques in recent years, the majority of the focus is rather handling how to generate the explanations, rather than understanding whether the explanations are accurate/reasonable, what if the explanations are inaccurate/unreasonable, and how to adjust the model to generate more accurate/reasonable explanations. \n\nTo explore and answer the above questions, this dissertation aims to explore a new line of research called Explanation-Guided Learning (EGL) that intervenes the deep learning models' behavior through XAI techniques to jointly improve DNNs in terms of both their explainability and generalizability. Particularly, we propose to explore the EGL on geometric data, including image and graph-structured data, which are currently under-explored in the research community due to the complexity and inherent challenges in geometric data explanation.\n\n\nTo achieve the above goals, we start by exploring the interpretability methods for geometric data on understanding the concepts learned by the deep neural networks (DNNs) with bio-inspired approaches and propose methods to explain the predictions of Graph Neural Networks (GNNs) on healthcare applications. Next, we design an interactive and general explanation supervision framework GNES for graph neural networks to enable the learning to explain pipeline, such that more reasonable and steerable explanations could be provided.  Finally, we propose two generic frameworks, namely GRADIA and RES, for robust visual explanation-guided learning by developing novel explanation model objectives that can handle the noisy human annotation labels as the supervision signal with a theoretical justification of the benefit to model generalizability.\n\n\nThis research spans multiple disciplines and promises to make general contributions in various domains such as deep learning, explainable AI, healthcare, computational neuroscience, and human-computer interaction by putting forth novel frameworks that can be applied to various real-world problems where both interpretability and task performance are crucial.", "imageUrl": "images/test.jpg"}
{ "id": "11", "title": "Expressive computation", "author": "Jennifer Jacobs, University of California, Santa Barbara", "date": "2022-11-18", "description": "Creators in many different fields use their hands. Artists and\ncraftspeople manipulate physical materials, manufacturers manually controlmachine tools, \nand designers sketch ideas. Computers are increasingly displacing\nmany manual practices in favor of procedural description and automated\nproduction. Despite this trend, computational and manual forms of creation are\nnot mutually exclusive. In this talk, I argue that by developing methods to\nintegrate computational and physical making, we can dramatically expand the\nexpressive potential of computers and broaden participation in computational\nproduction. To support this argument, I will present research across three\ncategories: 1) Integrating physical and manual creation with computer\nprogramming through domain-specific programming environments. 2) Broadening\nprofessional computational making through computational fabrication\ntechnologies. 3) Broadening entry points into computer science learning by\nblending programming with art, craft, and design. Collectively, my research\ndemonstrates how developing computational workflows, representations, and\ninterfaces for manual and physical making can enable manual creators to leverage\nexisting knowledge and skills. Furthermore, I’ll discuss how collaborating with\npractitioners from art, craft, and manufacturing science can diversify\napproaches to knowledge production in systems engineering and open new research\nopportunities in computer science.\n\n\nBio:\nJennifer Jacobs is Assistant Professor at the University of California Santa\nBarbara in Media Arts and Technology and Computer Science (by courtesy). At\nUCSB, she directs the Expressive Computation Lab, which investigates ways to\nsupport expressive computer-aided design, art, craft, and manufacturing by\ndeveloping new computational tools, abstractions, and systems that integrate\nemerging forms of computational creation and digital fabrication with\ntraditional materials, manual control, and non-linear design practices. Prior to\njoining UCSB, Jennifer received her Ph.D. from the Massachusetts Institute of\nTechnology and was a Postdoctoral Fellow at the Brown Institute of Media\nInnovation within the Department of Computer Science at Stanford University. She\nalso received an M.F.A. and B.F.A from Hunter College and the University of\nOregon respectively. Her research has been presented at leading human-computer\ninteraction research venues and journals including UIST, DIS, SIGGRAPH, and,\nmost prominently, at the flagship ACM Conference on Human Factors in Computing\nSystems (CHI).\n\n\nZoom Option:   https://emory.zoom.us/j/95719302738", "imageUrl": "images/test.jpg"}
{ "id": "12", "title": "Improving Interactive Search with User Feedback", "author": "Jianghong Zhou, Emory University", "date": "2022-11-17", "description": "Capturing users’ feedback can improve the interactive search. In search tasks, users\n\ntypically generate feedback while browsing the search results. That feedback may\n\ninclude clicking the items, reading important text content, query reformulation, and\n\nother interactions. They can reveal users’ latent intents and additional information\n\nneeds, providing essential extra information to improve users’ search experience.\n\nUnlike traditional search, the interactive search is enriched by more interactions\n\nand comprises three significant steps: Users browse the initial retrieval contents and\n\ngenerate feedback. The feedback is received and analyzed by the search system. The\n\nsearch system presents new search results based on users’ feedback. However, the\n\ncomplexity of human interactions challenges these three crucial steps when building\n\nan efficient interactive search engine.\n\n\n \n\nThe first challenge is obtaining informative and valuable feedback from the users.\n\nThis thesis introduces a new approach that can diversify the initial search results,\n\nallow users to explore multiple aspects of their original queries, and generate\n\ninstructive feedback. The approach is the first to use Simpson’s Diversity Index and Binary\n\nquadratic optimization in search diversification problems. Compared to the previous\n\nresearch, this method is more efficient and fast.\n\n \n\nAnother critical challenge is reducing the biased noise in the received feedback. In\n\nthis thesis, we propose a novel de-biased method to decrease the feedback’s high bias\n\ncaused by users’ observations. The approach uses a new observation mechanism to\n\nsimulate the users’ observation process and train a neural network model to detect\n\nthe observation bias. This new model outperforms the previous click models in both\n\nclick simulation and document ranking.\n\n \n\n\nThe last challenge is effectively extracting different interaction information and\n\nusing them to improve the search. In this thesis, we focus on document-level and\n\nsentence-level interactions. We propose two different approaches with reinforcement\n\nlearning frameworks. These methodologies introduce new techniques to reformulate\n\nthe query and rank the items. Both methods significantly improve the search performance\n\nin the interactive search process.\n\n \n\n\nTogether these techniques provide imperative solutions to the challenges in the\n\nthree critical steps of the interactive search systems and enable the users to obtain a\n\nbetter search experience.", "imageUrl": "images/test.jpg"}
{ "id": "13", "title": "Towards the Robustness of Deep Learning Systems Against Adversarial Examples in Sequential Data", "author": "Wenjie Wang, Emory University", "date": "2022-11-17", "description": "Recent studies have shown that adversarial examples can be generated by applying small perturbations to the inputs such that the well-trained deep neural networks (DNNs) will misclassify. With the increasing number of safety and security-sensitive applications of deep learning models, the robustness of deep learning models to adversarial inputs has become a crucial topic. Research on the adversarial examples in computer vision (CV) domains has been well studied. However, the intrinsic difference between image and sequential data has placed great challenges for directly applying adversarial techniques in CV to other application domains such as speech, health informatics, and natural language processing (NLP). \n\nTo solve these gaps and challenges, My dissertation research combines multiple studies to improve the robustness of deep learning systems against adversarial examples in sequential inputs. First, We take the NLP and health informatics domains as examples, focusing on understanding the characteristics of these two domains individually and designing empirical adversarial defense methods, which are 1) RADAR, an adversarial detection for EHR data, and  2) MATCH, detecting adversarial examples leveraging the consistency between multiple modalities. Following the empirical defense methods, our next step is exploring certified robustness for sequential inputs which is provable and theory-backed. To this end, 1) We study the randomized smoothing on the word embedding space to provide certification to NLP models. 2) We propose WordDP, certified robustness to word substitution attacks in the NLP domain, leveraging the connection of differential privacy and certified robustness. 3) We studied the certified robustness methods to Wasserstein adversarial examples on univariant time-series data.", "imageUrl": "images/test.jpg"}
{ "id": "14", "title": "The Applications of Alternating Minimization Algorithms on Deep Learning Models", "author": "Junxiang Wang, Emory University", "date": "2022-11-15", "description": "Gradient Descent(GD) and its variants are the most popular optimizers for training deep learning models. However, they suffer from many challenges such as gradient vanishing and poor conditioning, which prevent their more widespread use. To address these intrinsic drawbacks, alternating minimization methods have attracted attention from researchers as a potential way to train deep learning models. Their idea is to decompose a neural network into a series of linear and nonlinear equality constraints, which generate multiple subproblems and they can be minimized alternately. Their empirical evaluations demonstrate good scalability and high accuracy. They also avoid gradient vanishing problems and allow for non-differentiable activation functions, as well as allowing for complex non-smooth regularization and the constraints that are increasingly important for neural network architectures.\n\nThis dissertation aims to develop alternating minimization methods to train the Multi-Layer Perceptron(MLP) model. This includes deep learning Alternating Direction Method of Multipliers(dlADMM), monotonous Deep Learning Alternating Minimization(mDLAM), and parallel deep learning Alternating Direction Method of Multipliers(pdADMM). The extended pdADMM-G algorithm and the pdADMM-G-Q algorithms are developed to train the Graph-Augmented Multi-Layer Perceptron(GA-MLP) model.\n\nFor the dlADMM algorithm, parameters in each layer are updated in a backward and forward fashion. The time complexity is reduced from cubic to quadratic in(latent) feature dimensions for subproblems by iterative quadratic approximations and backtracking. Finally, we provide the convergence guarantee of the dlADMM algorithm under mild conditions.\n\nFor the mDLAM algorithm, our innovative inequality-constrained formulation infinitely approximates the original problem with non-convex equality constraints, enabling our convergence proof of the proposed mDLAM algorithm regardless of the choice of hyperparameters. Our mDLAM algorithm is shown to achieve a fast linear convergence by the Nesterov acceleration technique.\n\nFor the pdADMM algorithm, we achieve model parallelism by breaking layer dependency: parameters in each layer of neural networks can be updated independently in parallel. The convergence of the proposed pdADMM to a stationary point is theoretically proven under mild conditions. The convergence rate of the pdADMM is proven to be $o(1/k)$, where $k$ is the number of iterations.\n\nFor the pdADMM-G algorithm and the pdADMM-G-Q algorithm, in order to achieve model parallelism, we extend the proposed pdADMM algorithm to train the GA-MLP model, named the pdADMM-G algorithm. The extended pdADMM-G-Q algorithm reduces communication costs by introducing the quantization technique. Theoretical convergence to a (quantized) stationary point of two proposed algorithms is provided with a sublinear convergence rate $o(1/k)$, where $k$ is the number of iterations.", "imageUrl": "images/test.jpg"}
{ "id": "15", "title": "Towards Designing Inclusive Social Virtual Reality Spaces to Combat New Forms of Online Harassment", "author": "Guo Freeman, Clemson University", "date": "2022-11-11", "description": "Social Virtual Reality refers to 3D virtual spaces where multiple users can interact with one another through VR head-mounted displays. In recent years, the growing popularity of commercial social VR platforms such as AltspaceVR, VR Chat, RecRoom, and Meta Horizon Worlds is dramatically transforming how people meet, interact, play, and collaborate online and has led to the emerging metaverse paradigm. These platforms have drawn aspects from traditional multiplayer online games and 3D virtual worlds where users engage in various immersive experiences, interactive activities, and choices through avatar-based online representations. However, social VR also demonstrates specific nuances, including full/partial body tracked avatars, synchronous voice conversations, and simulated touching and grabbing features. These novel characteristics have led to greater instances of harassment and potentially more destructive consequences compared to traditional 3D virtual worlds/online gaming or single-user VR. In this talk, Dr. Guo Freeman will introduce her recent research on new forms of online harassment in social VR and how embodied harassment is becoming an emerging but understudied form of harassment in novel online social spaces. She will explain her ongoing work on leveraging innovative technologies, such as AI-based moderation, for proactively combating harassment in social VR. She will also highlight potential future directions for designing safer, inclusive, and more supportive social VR spaces to empower diverse communities, especially marginalized users such as women, ethnic minorities, and LGBTQ individuals.  \n\n\nBio:  Dr. Guo Freeman is an Assistant Professor of Human-Centered Computing in the School of Computing at Clemson University. Her research situates at the unique intersection of social computing, social VR, and entertainment computing. She brings a combination of profound theoretical foundation, nuanced empirical perspectives, and participatory technology design and prototype to investigate how interactive technologies such as multiplayer online games, esports, live streaming, social VR, social media, and AI shape interpersonal relationships and group behavior. Her research is also uniquely driven by her focus on marginalized technology users due to their gender, race, sexuality, age, and disability, including women, LGBTQ individuals, ethnic minorities, minors, and persons with disabilities. At Clemson, she leads the Gaming and Mediated Experience Lab (CUGAME). She has authored over 80 peer-reviewed publications and won multiple Best Paper Honorable Mentions (top 5%) at CHI, CSCW, and iConference. She has secured $20.4 million in external grant funding from the National Science Foundation (NSF), Air Force Office of Scientific Research (AFOSR), and US Army, with $1.77 million dedicated to her effort. She is a member of the ACM CHI PLAY Steering Committee and has served on over 18 Program Committees for prestigious international HCI venues such as CHI, CSCW, and CHIPLAY. She especially dedicates to broadening women’s and minorities’ participation in computing and was a Grace Hopper Women in Computing Faculty Mentor.", "imageUrl": "images/test.jpg"}
{ "id": "16", "title": "Learning Movement Representations of Small Humans with Small Data", "author": "Sarah Ostadabbas, Northeastern University", "date": "2022-11-04", "description": "Closely tracking the development of motor functioning in infants provides prodromal risk markers of many developmental disruption such as autism spectrum disorder (ASD), cerebral palsy (CP), and developmental coordination disorder (DCD), among others. Screening for motor delays will allow for earlier and more targeted interventions that will have a cascading effect on multiple domains of infant development, including communication, social, cognitive, and memory. However, only about 29% of US children under 5 years of age receive developmental screening due to expense and shortage of testing resources, contributing negatively to lifelong outcomes for infants at risk for developmental delays. My research aims to learn and quantify visual representations of  motor function in infants towards designing an accessible and affordable video-based screening technology for their motor skills by developing novel data-/label-efficient AI techniques including biomechanically-constrained synthetic data augmentation, semantic-aware domain adaptation, and human-AI co-labeling algorithms. \n\nWhile there are several powerful human behavior recognition and tracking algorithms, however, models trained on large-scale adult activity datasets have limited success in estimating infant movements due to the significant differences in their body ratios, the complexity of infant poses, and types of their activities. Privacy and security considerations hinder the availability of adequate infant images/videos required for training of a robust model with deep structure from scratch, making this a particularly constrained ``small data problem''. To address this gap, in this talk I will cover: (i) introduction of biomechanically-constrained models to synthesize labeled pose data in the form of domain-adjacent data augmentation; (ii) design and analysis of a semantic-aware unsupervised domain adaptation technique to close the gap between the domain-adjacent and domain-specific pose data distributions; and (iii) development and analysis of an AI-human co-labeling technique to provide high-quality labels to refine and adapt the domain-adapted inference models into  robust pose estimation algorithms in the target application.  These contributions enable the use of advanced AI in the small data domain.\n\nZoom Option:   https://emory.zoom.us/j/95719302738", "imageUrl": "images/test.jpg"}
{ "id": "17", "title": "Bias and XR", "author": "Tabitha Peck, Davidson College", "date": "2022-10-28", "description": "Bias, a prejudice in favor of or against one group compared to another, affects\nour lives from how we act to how things are designed. A person's biases can\ncause harm, such as a doctor's implicit bias when treating a patient or a\nteacher's implicit bias when working with students. Augmented, Mixed, and\nVirtual Reality are power technologies that can be used to study human behavior.\nIn this talk I will present ways in which these XR technologies can be used to\ninvestigate and mitigate harmful biases. However, XR has been created by humans\nand I will further discuss ways that design biases have been added into these\nsystems. I will conclude with a call to action to researchers providing\nactionable steps to help mitigate bias within our research practices.", "imageUrl": "images/test.jpg"}
{ "id": "18", "title": "Computational Image Processing and Deep Learning with Multi-Model Biomedical Image Data", "author": "Hanyi Yu, Emory University", "date": "2022-10-24", "description": "With the rapid advance in medical imaging technology in recent decades, computational image analysis has become a popular research topic in the field of biomedical informatics. Images from various imaging acquisition platforms have been widely used for the early detection, diagnosis, and treatment response assessment in a large number of disease and cancer studies. Although computational methods present higher analysis efficiency and less variability than manual analyses, they require appropriate parameter settings to achieve optimal results. This can be demanding for medical researchers lacking relevant knowledge about computational method development. In the last decade, deep neural networks trained on large-scale labeled datasets have provided a promising and convenient end-to-end solution to biomedical image processing. However, the development of deep-learning tools for biomedical image analysis is often restrained by inadequate data with high-quality annotations in practice. By contrast, a large number of unlabeled biomedical images are generated by daily research and clinical activities. Thus, leveraging unlabeled images with semi-supervised or even unsupervised deep learning approaches has become a significant research direction in biomedical informatics analysis. \n\n\n\nMy primary doctoral research focuses on the field of medical image processing, utilizing computational methods to facilitate biomedical image analysis with limited supervision. I have explored two ways to achieve this primarily: (1) Optimizing the model of existing approaches for specific tasks and (2) Developing semi-supervised/unsupervised deep learning approaches. In my research, I mainly focus on image segmentation and object tracking, two common biomedical image analysis tasks. By experimenting with different types of images (e.g., fluorescence microscopy images and histopathology microscopy images) from various sources (e.g., bacteria, human liver biopsies, and retinal pigment epithelium tissues), my developed methods demonstrate their promising potential to support biomedical image analysis tasks. \n\n\nZoom link: Join Zoom Meeting\n\n\nhttps://emory.zoom.us/j/97736956694", "imageUrl": "images/test.jpg"}
{ "id": "19", "title": "Relating enhancer genetic variation across mammals to complex phenotypes using machine learning", "author": "Irene Kaplow, Carnegie Mellon University", "date": "2022-10-21", "description": "Many mammalian characteristics have evolved multiple times throughout history. For example, humans and dolphins have larger brains relative to body size than their close relatives, chimpanzees and killer whales. We want to identify the parts of the genome associated with these characteristics by comparing the genomes of hundreds of mammals. Rather than focusing on the small proportion of the genome that encodes genes, which cannot fully explain many of characteristics’ evolution, we present a new approach that uses machine learning to find conserved patterns of sequences at candidate enhancer regions, which control the levels of genes expressed in specific tissues. We established a new set of evaluation criteria for these machine learning models and used these criteria to compare our models to previous methods for this task. When applying our approach to the brain, we identified dozens of new enhancers associated with the evolution of brain size relative to body size and vocal learning.\n\nBio:\nIrene Kaplow received her B.S. in Mathematics with a minor in Biology from the Massachusetts Institute of Technology in 2010. There, she began her career as a computational biologist while doing research with Bonnie Berger. She then went to graduate school at Stanford University, where she received her Ph.D. in Computer Science in 2017. At Stanford, she worked in the Hunter Fraser and Anshul Kundaje's labs to develop methods to analyze novel high-throughput sequencing datasets to better understand the roles of DNA methylation and Cys2-His2 zinc finger transcription factor binding in gene expression regulation. Irene is now a Lane Postdoctoral Fellow in Andreas Pfenning's lab in the Computational Biology Department at Carnegie Mellon University, where she is developing methods to identify enhancers involved in the evolution of neurological characteristics that have evolved through gene expression.", "imageUrl": "images/test.jpg"}
{ "id": "20", "title": "Towards the development of adaptive and adaptable multimodal displays", "author": "Sara Riggs, PhD, University of Virginia", "date": "2022-10-14", "description": "Data-rich environments, such as aviation, military operations, and\nmedicine, impose considerable and continually increasing attentional demands on\noperators by requiring them to divide their mental resources effectively amongst\nnumerous tasks and sources of information. Data overload, especially in the\nvisual channel, and associated breakdowns in monitoring represent a major\nchallenge in these environments. One promising means of overcoming data overload\nis through the introduction of multimodal displays, i.e., displays which\ndistribute information across various sensory channels (including vision,\naudition, and touch). However, several questions remain unanswered regarding the\ndesign and limitations of this approach. In this talk, I will summarize two\nongoing research efforts that seek to answer the following: (1) how movement\naffects sensory perception and performance in the real world and virtual reality\nand (2) how workload transitions affect performance and visual attention\nallocation. In combination, the results from these two ongoing research efforts\nwill help inform design guidelines for adaptive and/or adaptable multimodal\ndisplays that can adjust the nature of information presentation in response to\nthe user in a context-sensitive fashion.\n\nBio: Sara Riggs is an associate Professor and Assistant Chair of Research and\nDevelopment in the Department of Engineering Systems and Environment at the\nUniversity of Virginia. She received her PhD and MSE in Industrial and\nOperations Engineering from the University of Michigan. Her research focuses on\ntask sharing, attention management, and interruption management in complex\nenvironments that have included aviation, healthcare, and military operations.\nShe has ongoing research in the areas of: (a) multimodal display design, (b)\ncognitive processing limitations, and (c) adaptive/adaptable display design. Her\nresearch been funded by the NSF, AHRQ, NIH, and AFOSR. She is the recipient of\nthe NSF CAREER Award, 2019 Jerome H. Ely Human Factors Article Award, and 2016\nAPA Briggs Dissertation Award.", "imageUrl": "images/test.jpg"}
{ "id": "21", "title": "Medical Image Analysis with Deep learning under Limited Supervision", "author": "Xiaoyuan Guo, Emory University", "date": "2022-10-12", "description": "Medical imaging plays a significant role in different clinical applications such as detection, monitoring, diagnosis, and treatment evaluation of various clinical conditions. Deep learning approach for medical image analysis emerged as a fast-growing research field and has been widely used to facilitate challenging image analysis tasks, for example, detecting the presence or absence of a particular abnormality, diagnosis of a particular tumor subtype. However, one important requisite is the large amount of annotated data for supervised training, which is often lacking in medicine due to the expensive and time-consuming expert-driven data curation process. Data insufficiency in medical images is also limited by healthcare data privacy requirements, which leads to barriers in the usage of deep learning methods across institutions.\n\n\nThis thesis focuses on facilitating the applications of deep learning approaches to solve automatic medical image analysis tasks efficiently under limited supervision.  Three situations are in consideration: (1) no annotated data; (2) limited annotated data; (3) curation of additional annotated data with minimal human supervision.  The research covers multiple medical image modalities starting from fluorescence microscopy images (FMI), histopathological microscopy images (HMI) to mammogram images (MG), computed tomography (CT), chest radiographs (X-ray). The researched tasks are diverse including image segmentation, Out-of-Distribution (OOD) identification and medical image retrieval. The diversity and concreteness of the thesis can be a guide to facilitate the efficient usage of deep learning approaches in future medical image analysis with minimal cost.", "imageUrl": "images/test.jpg"}
{ "id": "22", "title": "Disseminating Health Informatics/Data Science Innovations", "author": "Suzanne Bakken, PhD, RN, FAAN, FACMI, FIAHSI, Columbia University", "date": "2022-10-07", "description": "Suzanne Bakken, PhD, RN, FAAN, FACMI, FIAHSI, is the Alumni Professor of Nursing and Professor of Biomedical Informatics at Columbia University. Following her doctorate in Nursing at the University of California, San Francisco, she completed a post-doctoral fellowship in Medical Informatics at Stanford University. Her program of research has focused on the intersection of informatics and health equity for more than 30 years and has been funded by AHRQ, NCI, NIMH, NINR, and NLM. Dr. Bakken’s program of research has resulted in > 300 peer-reviewed papers. She is a Fellow of the American Academy of Nursing, American College of Medical Informatics, International Academy of Health Sciences Informatics, and a member of the National Academy of Medicine. Dr. Bakken has received multiple awards for her research including the Pathfinder Award from the Friends of the National Institute of Nursing Research, the Nursing Informatics Award from the Friends of the National Library of Medicine, the Sigma Theta Tau International Nurse Researchers Hall of Fame, the Virginia K. Saba Award from the American Medical Informatics Association, and the Francois Gremy Award from the International Medical Informatics Association. Dr. Bakken currently serves as Editor-in-Chief of the Journal of the American Medical Informatics Association.", "imageUrl": "images/test.jpg"}
{ "id": "23", "title": "Characterization and Mitigation of Misinformation in Social Media Characterization and Mitigation of Misinformation in Social Media", "author": "Francesca Spezzano, PhD, Boise State University", "date": "2022-09-30", "description": "Social media and Web sources have made information available, accessible, and shareable anytime and anywhere nearly without friction. This information can be truthful, falsified, or can only be the opinion of the writer as users on such platforms are both information creators and consumers. In any case, it has the power to affect the decision of an individual, the beliefs of the society, activities, and the economy of the whole country. Thus, it is imperative to identify misinformation and mitigate the effects of false information that are ubiquitous across the Web and social media. In this talk, we first analyze the reasons behind the success of misinformation, then, we present ways of identifying misinformation and the actors responsible for spreading it, and finally, we analyze novel ways to model misinformation diffusion.", "imageUrl": "images/test.jpg"}
{ "id": "24", "title": "Crowd Sleuths", "author": "Kurt Luther, Virginia Tech", "date": "2022-09-16", "description": "Crowd Sleuths: Solving Mysteries with Crowdsourcing, Experts, and AI \n \nProfessional investigators, such as journalists and police detectives, have long sought the public's help in solving mysteries, typically by \nsoliciting tips. However, as social technologies mediate more aspects of daily life and enable new forms of collaboration, members of the public are increasingly leading their own investigations, with mixed results. In this talk, I present three projects from my research group, the Crowd Intelligence Lab, where we build software tools that bring together crowds, experts, and AI to support ethical and effective investigations and solve mysteries. In the CrowdIA project, we adapted the sense making loop for intelligence analysts to enable novice crowds to discover a hidden terrorist plot within large quantities of textual evidence documents. In \nthe GroundTruth project, we developed a novel diagramming technique to enable novice crowds to collaborate with expert investigators to verify (or debunk) photos and videos shared on social media. In the Photo Sleuth project, we built and launched a free public website with over 10,000 registered users who employ AI-based face recognition to identify unknown soldiers in historical portraits from the American Civil War era. I will conclude the talk by discussing broader opportunities and risks in combining the complementary strengths of human and artificial intelligence for investigation, sense making, and other complex and creative tasks.", "imageUrl": "images/test.jpg"}